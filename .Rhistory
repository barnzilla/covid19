"tidyr",
"tidytext",
"tidyverse",
"topicmodels",
"wordcloud"
)
load_packages <- lapply(package_names, require, character.only = TRUE)
# Chunk 3
capitalize_first_letter <- function(x) {
s <- strsplit(x, " ")[[1]]
paste(toupper(substring(s, 1,1)), substring(s, 2), sep = "", collapse = " ")
}
get_topics <- function(v, n, bigram = FALSE) {
text_df <- tibble(line = 1:length(v), text = v)
if(isTRUE(bigram)) {
#number_of_columns = 2
text_df <- text_df %>%
unnest_tokens(word, text, token = "ngrams", n = 2)
text_df$has_stop_word <- sapply(text_df$word, function(x) {
if(sum(strsplit(x , " ")[[1]] %in% stop_words$word) < 1) 0 else 1
})
text_df <- text_df[text_df$has_stop_word == 0,]
text_df <- text_df[text_df$word != "rights reserved",]
} else {
#number_of_columns = 3
text_df <- text_df %>%
unnest_tokens(word, text)
text_df <- text_df %>%
anti_join(stop_words)
}
# cast into a Document-Term Matrix
text_df2 <- text_df %>%
count(line, word, sort = TRUE)
text_df2 <- text_df2 %>%
cast_dtm(line, word, n)
# set a seed so that the output of the model is predictable
lda <- LDA(text_df2, k = n, control = list(seed = 1234))
topics <- tidy(lda, matrix = "beta")
top_terms <- topics %>%
group_by(topic) %>%
top_n(7, beta) %>%
ungroup() %>%
arrange(topic, -beta)
colors <- colorRampPalette(c("#4e2784", "#f36621"))
p <- top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_minimal() +
scale_fill_manual(values = colors(n))
if(isTRUE(bigram) & n > 5) {
p <- p +
xlab("Top bigrams per topic") +
ylab("Per topic per bigram probabilities") +
theme(axis.text.y = element_text(size = 6))
} else {
p <- p +
xlab("Top words per topic") +
ylab("Per topic per word probabilities")
}
p + theme(axis.text.x = element_text(size = 6))
}
# Chunk 4
df <- read.csv("scraped-data.csv", stringsAsFactors = FALSE)
df <- read.csv("c:/users/joelb/onedrive/github/participaction-report-card/scraped-data.csv", stringsAsFactors = FALSE)
# Convert abstracts to a tibble
text_df <- tibble(line = 1:nrow(df), text = df$title)
# Tokenize
text_df <- text_df %>%
unnest_tokens(word, text)
# Remove stop words
## Load stop words
data(stop_words)
## Add some words to stop words
additional_stop_words <- c("95", "ci", "202f", "2", "1", "3", "5", "6", "12", "10", "4", "9", "ÃŸ", "8", "11", "7", "cross", "sectional", "0.001", "2018", "24", "18", "2019", "13", "15", "0.05", "14", "2017", "20", "17", "0", "30", "2015", "0.01", "i.e", "e.g", "60", "16", "2014", "19", "2016", "25", "2013", "36", "05", "001", "0.02", "50", "21", "2011", "0.03", "23", "2012", "0.04", "se", "48", "99", "ii", "28", "31", "26", "49", "m2", "34", "37", "1.0", "35", "iii", "2009", "mt")
stop_words <- add_row(
stop_words,
word = additional_stop_words,
lexicon = rep("Joel", length(additional_stop_words))
)
text_df <- text_df %>%
anti_join(stop_words)
# Plot the most common words
plot_df <- text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 50) %>%
mutate(word = reorder(word, n))
# Render plot
ggplot(
data = plot_df,
aes(x = word, y = n)) +
geom_bar(stat = "identity", position = position_dodge(), fill = "#f36621") +
theme_minimal() +
geom_text(
label = sort(plot_df$n, decreasing = FALSE),
position = position_dodge(width = 0.9),
vjust = 1.25,
color = "white",
fontface = "bold",
size = 3
) +
labs(x = "", y = "Frequency") +
theme(
text = element_text(size = 11),
legend.position = "top",
axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()
) +
theme(
legend.text = element_text(size = 8),
legend.title = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)
)
# Render table
kable(
text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 25) %>%
mutate(word = reorder(word, n)) %>%
setNames(., c("Word", "Frequency"))
) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Save titles for sentiment analysis
text_df_titles <- text_df
# Convert abstracts to a tibble
text_df <- tibble(line = 1:nrow(df), text = df$abstract)
# Tokenize
text_df <- text_df %>%
unnest_tokens(word, text)
## Add some words to stop words
text_df <- text_df %>%
anti_join(stop_words)
# Plot the most common words
plot_df <- text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 400) %>%
mutate(word = reorder(word, n))
# Render plot
ggplot(
data = plot_df,
aes(x = word, y = n)) +
geom_bar(stat = "identity", position = position_dodge(), fill = "#f36621") +
theme_minimal() +
geom_text(
label = sort(plot_df$n, decreasing = FALSE),
position = position_dodge(width = 0.9),
vjust = 1.25,
color = "white",
fontface = "bold",
size = 3
) +
labs(x = "", y = "Frequency") +
theme(
text = element_text(size = 11),
legend.position = "top",
axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()
) +
theme(
legend.text = element_text(size = 8),
legend.title = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)
)
# Render table
kable(
text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 25) %>%
mutate(word = reorder(word, n)) %>%
setNames(., c("Word", "Frequency"))
) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Save abstracts for sentiment analysis
text_df_abstracts <- text_df
# Convert abstracts to a tibble
text_df <- tibble(line = 1:nrow(df), text = df$authors)
# Tokenize
text_df <- text_df %>%
unnest_tokens(word, text)
## Add some words to stop words
text_df <- text_df %>%
anti_join(stop_words)
text_df <- subset(text_df, nchar(as.character(word)) > 2)
text_df <- text_df[text_df$word != "van",]
# Plot the most common words
plot_df <- text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 15) %>%
mutate(word = reorder(word, n))
# Render plot
ggplot(
data = plot_df,
aes(x = word, y = n)) +
geom_bar(stat = "identity", position = position_dodge(), fill = "#f36621") +
theme_minimal() +
geom_text(
label = sort(plot_df$n, decreasing = FALSE),
position = position_dodge(width = 0.9),
vjust = 1.25,
color = "white",
fontface = "bold",
size = 3
) +
labs(x = "", y = "Frequency") +
theme(
text = element_text(size = 11),
legend.position = "top",
axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()
) +
theme(
legend.text = element_text(size = 8),
legend.title = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)
)
# Render table
kable(
text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 25) %>%
mutate(word = reorder(word, n)) %>%
setNames(., c("Word", "Frequency"))
) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Convert abstracts to a tibble
text_df <- tibble(line = 1:nrow(df), text = df$title)
# Tokenize
text_df <- text_df %>%
unnest_tokens(word, text, token = "ngrams", n = 2)
# Remove bigrams with stop words
text_df$has_stop_word <- sapply(
text_df$word,
function(x) if(sum(strsplit(x , " ")[[1]] %in% stop_words$word) < 1) 0 else 1
)
text_df <- text_df[text_df$has_stop_word == 0,]
# Plot the most common words
plot_df <- text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 15) %>%
mutate(word = reorder(word, n))
# Render plot
ggplot(
data = plot_df,
aes(x = word, y = n)) +
geom_bar(stat = "identity", position = position_dodge(), fill = "#f36621") +
theme_minimal() +
geom_text(
label = sort(plot_df$n, decreasing = FALSE),
position = position_dodge(width = 0.9),
vjust = 1.25,
color = "white",
fontface = "bold",
size = 3
) +
labs(x = "", y = "Frequency") +
theme(
text = element_text(size = 11),
legend.position = "top",
axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()
) +
theme(
legend.text = element_text(size = 8),
legend.title = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)
)
# Render table
kable(
text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 25) %>%
mutate(word = reorder(word, n)) %>%
setNames(., c("Word", "Frequency"))
) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Save for sentiment analysis
text_df_bigram_titles <- text_df
text_df_titles %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(
colors = c("#f36621", "#4e2784"),
max.words = 200,
title.colors = c("white", "white"),
title.bg.colors = c("#f36621", "#4e2784")
)
bing_word_counts <- text_df_titles %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts$sentiment <- sapply(
bing_word_counts$sentiment,
function(x) capitalize_first_letter(x)
)
bing_word_counts %>%
group_by(sentiment) %>%
top_n(25) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment", x = NULL) +
coord_flip() +
theme_minimal() +
scale_fill_manual(values=c("#f36621", "#4e2784"))
text_df_titles %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(
colors = c("#f36621", "#4e2784"),
max.words = 200,
title.colors = c("white", "white"),
title.bg.colors = c("#f36621", "#4e2784")
)
bing_word_counts <- text_df_titles %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts$sentiment <- sapply(
bing_word_counts$sentiment,
function(x) capitalize_first_letter(x)
)
bing_word_counts %>%
group_by(sentiment) %>%
top_n(25) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment", x = NULL) +
coord_flip() +
theme_minimal() +
scale_fill_manual(values=c("#f36621", "#4e2784"))
bing_word_counts %>%
group_by(sentiment) %>%
top_n(25) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment", x = NULL) +
coord_flip() +
theme_minimal() +
#scale_fill_manual(values=c("#f36621", "#4e2784"))
```
bing_word_counts %>%
group_by(sentiment) %>%
top_n(25) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment", x = NULL) +
coord_flip() +
theme_minimal()
text_df_abstracts %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(
colors = c("#f36621", "#4e2784"),
max.words = 200,
title.colors = c("white", "white"),
title.bg.colors = c("#f36621", "#4e2784")
)
bing_word_counts <- text_df_abstracts %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts$sentiment <- sapply(
bing_word_counts$sentiment,
function(x) capitalize_first_letter(x)
)
bing_word_counts %>%
group_by(sentiment) %>%
top_n(25) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment", x = NULL) +
coord_flip() +
theme_minimal()
text_df_abstracts %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(
colors = c("#f36621", "#4e2784"),
max.words = 200,
title.colors = c("white", "white"),
title.bg.colors = c("#f36621", "#4e2784")
)
bing_word_counts <- text_df_abstracts %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts$sentiment <- sapply(
bing_word_counts$sentiment,
function(x) capitalize_first_letter(x)
)
bing_word_counts %>%
group_by(sentiment) %>%
top_n(25) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment", x = NULL) +
coord_flip() +
theme_minimal()
# Convert abstracts to a tibble
text_df <- tibble(line = 1:nrow(df), text = df$title)
# Tokenize
text_df <- text_df %>%
unnest_tokens(word, text)
# Remove stop words
## Load stop words
data(stop_words)
## Add some words to stop words
additional_stop_words <- c("95", "ci", "202f", "2", "1", "3", "5", "6", "12", "10", "4", "9", "ÃŸ", "8", "11", "7", "cross", "sectional", "0.001", "2018", "24", "18", "2019", "13", "15", "0.05", "14", "2017", "20", "17", "0", "30", "2015", "0.01", "i.e", "e.g", "60", "16", "2014", "19", "2016", "25", "2013", "36", "05", "001", "0.02", "50", "21", "2011", "0.03", "23", "2012", "0.04", "se", "48", "99", "ii", "28", "31", "26", "49", "m2", "34", "37", "1.0", "35", "iii", "2009", "mt")
stop_words <- add_row(
stop_words,
word = additional_stop_words,
lexicon = rep("Joel", length(additional_stop_words))
)
text_df <- text_df %>%
anti_join(stop_words)
# Plot the most common words
plot_df <- text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 50) %>%
mutate(word = reorder(word, n))
# Render plot
ggplot(
data = plot_df,
aes(x = word, y = n)) +
geom_bar(stat = "identity", position = position_dodge(), fill = "#f36621") +
theme_minimal() +
geom_text(
label = sort(plot_df$n, decreasing = FALSE),
position = position_dodge(width = 0.9),
vjust = 1.25,
color = "white",
fontface = "bold",
size = 3
) +
labs(x = "", y = "Frequency") +
theme(
text = element_text(size = 11),
legend.position = "top",
axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()
) +
theme(
legend.text = element_text(size = 8),
legend.title = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)
)
# Render table
kable(
text_df %>%
count(word, sort = TRUE) %>%
filter(n >= 25) %>%
mutate(word = reorder(word, n)) %>%
setNames(., c("Word", "Frequency"))
) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Save titles for sentiment analysis
text_df_titles <- text_df
# Convert abstracts to a tibble
text_df <- tibble(line = 1:nrow(df), text = df$title)
text_df
str(text_df)
library(lubridate)
Sys.Date() - wday(Sys.Date() + 1)
wday(Sys.Date())
Sys.Date()
Sys.Date() + 1
wday(-1)
wday(Sys.Date() + 1)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(dplyr)
d <- tibble(
module = c("Module A", "Module B", "Module C"),
comments = c(100, 110, 120)
)
?sprintf
sapply(unique(d$module), function(x) {
cat(sprintf("## %s", x))
print("\n\n")
})
?cumsum
a = c("a", "b", "c")
cumsum(a)
