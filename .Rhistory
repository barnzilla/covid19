# Rename vectors with problematic names
names(dictionary)[c(5, 11, 12)] <- c("round_one", "abcd", "local_apk")
names(dictionary) <- gsub("|[[:digit:]]+", "", names(dictionary))
names(dictionary) <- gsub(" [(]| |-", "_", tolower(names(dictionary)))
names(dictionary) <- gsub("[)]", "", names(dictionary))
names(dictionary) <- gsub("\\_$", "", names(dictionary))
# Remove first two rows
dictionary <- dictionary[-c(1:2),]
# Remove select vectors
dictionary <- dictionary %>% select(-one_of(c("round_one", "local_apk")))
# Set elements 27, 39 and 45 to NA (duplicates) in the subgenre vector
dictionary$subgenre <- ifelse(dictionary$subgenre %in% c(27, 39, 45), NA, dictionary$subgenre)
# Coerce the minutes and props vectors to numeric objects
dictionary[c("minutes", "props")] <- lapply(dictionary[c("minutes", "props")], as.numeric)
# Coerce all but the first four vectors to integer objects
dictionary[-c(1:4)] <- lapply(dictionary[-c(1:4)], as.integer)
# Create a list of vectors that need to be created
vectors_to_create <- unname(unlist(sapply(names(dictionary)[-c(1:4)], function(x) {
names <- paste0(x, "_", sort(unique(eval(parse(text = paste0("dictionary$", x))))))
})))
# Chunk 6
# Rename vectors
names(categories) <- unlist(categories[1,])
# Rename vectors with problemtic names
names(categories)[4:6] <- c("games", "subgenre", "rating")
names(categories) <- gsub("|[[:digit:]]+", "", names(categories))
names(categories) <- gsub(" [(]| |-", "_", tolower(names(categories)))
names(categories) <- gsub("[)]", "", names(categories))
names(categories) <- gsub("\\_$", "", names(categories))
# Remove the first row and the third vector
categories <- categories[-c(1), -c(3)]
# Convert elements 27, 39 and 45 in the subgenre vector to NA (duplicates)
categories$subgenre[c(27, 39, 45)] <- NA
# Create a lookup table
variables <- labels <- c()
for(i in 1:ncol(categories)) {
for(j in 1:nrow(categories)) {
if(! is.na(categories[j, i])) {
explode <- strsplit(unlist(categories[j,i]), " - ")
variables <- append(variables, paste0(names(categories[i]), "_", explode[[1]][1]))
labels <- append(labels, gsub("[/]", "_", explode[[1]][2:(length(explode[[1]]))]))
}
}
}
lookup <- tibble(variables, labels)
# Coerce timestamp vectors to date objects
d[paste0("timestamp_", c("start", "stop"))] <- lapply(d[paste0("timestamp_", c("start", "stop"))], as.Date)
# Add vectors to the raw data corresponding to all of Michelle's categories
d[vectors_to_create] <- rep(NA, nrow(d))
# Iterate through these new vectors and update with foreground minutes if a code is found
for(i in 7:length(names(d))) {
explode <- strsplit(names(d)[i], "_")
variable <- paste0(explode[[1]][1:length(explode[[1]]) - 1], collapse = "_")
code <- as.integer(explode[[1]][length(explode[[1]])])
apps <- unname(unlist(dictionary %>% filter(!! sym(variable) == code) %>% select(app)))
eval(parse(text = paste0("d$", names(d)[i], " <- ifelse(d$app_id %in% apps, d$foreground_minutes, 0)")))
}
# Create aggregate raw data
d2 <-
bind_cols(
d %>%
group_by(participant_id) %>%
summarize(
start_date = min(timestamp_start),
stop_date = max(timestamp_stop),
days_of_data = length(unique(c(timestamp_start, timestamp_stop))),
daily_foreground_hours = round(sum(foreground_minutes) / 60 / days_of_data, 1),
),
d %>%
group_by(participant_id) %>%
summarise_at(vars(one_of(vectors_to_create)), sum) %>%
select(-participant_id)
)
# Create social media vectors
d2$Youtube <- sapply(unique(d$participant_id), function(x) get_minutes("com.google.android.youtube", x) )
d2$Tiktok <- sapply(unique(d$participant_id), function(x) get_minutes("com.zhiliaoapp.musically", x) )
d2$Snapchat <- sapply(unique(d$participant_id), function(x) get_minutes("com.snapchat.android", x) )
d2$Instagram <- sapply(unique(d$participant_id), function(x) get_minutes("com.instagram.android", x) )
d2$Netflix <- sapply(unique(d$participant_id), function(x) get_minutes("com.netflix.mediaclient", x) )
d2$Reddit <- sapply(unique(d$participant_id), function(x) get_minutes("com.reddit.frontpage", x) )
# Add ABCD demographic variables
abcd_demographics <- abcd_demographics[abcd_demographics$subjectkey %in% d2$participant_id,]
abcd_demographics <- abcd_demographics[match(d2$participant_id, abcd_demographics$subjectkey),]
abcd_demographics$demo_comb_income_v2 <- sapply(as.integer(as.character(abcd_demographics$demo_comb_income_v2)), function(x) to_na(x))
abcd_demographics$demo_prnt_ed_v2 <- sapply(as.integer(as.character(abcd_demographics$demo_prnt_ed_v2)), function(x) to_na(x))
abcd_demographics$demo_prtnr_ed_v2 <- sapply(as.integer(as.character(abcd_demographics$demo_prtnr_ed_v2)), function(x) to_na(x))
abcd_passive_sensing <- abcd_passive_sensing[abcd_passive_sensing$subjectkey %in% d2$participant_id,]
abcd_passive_sensing <- abcd_passive_sensing[match(d2$participant_id, abcd_passive_sensing$subjectkey),]
d2$Age <- as.integer(abcd_passive_sensing$interview_age) / 12
d2$Gender <- abcd_passive_sensing$sex
d2$`Family income` <- abcd_demographics$demo_comb_income_v2
d2$`Parent education` <- abcd_demographics$demo_prnt_ed_v2
d2$`Partner education` <- abcd_demographics$demo_prtnr_ed_v2
d2$`Max parental education` <- apply(
d2[c("Parent education", "Partner education")],
1,
function(x) {
if(sum(!is.na(x[1:2]), na.rm = TRUE) < 1 ) output <- NA else output <- max(x[1:2], na.rm = TRUE)
})
# Add ABCD self-reported screen time questions
abcd_screen_time <- abcd_screen_time[abcd_screen_time$subjectkey %in% d2$participant_id,]
abcd_screen_time <- abcd_screen_time[match(d2$participant_id, abcd_screen_time$subjectkey),]
abcd_screen_time[c("screentime_wkdy_typical_hr", "screentime_wkdy_typical_min", "screentime_wknd_typical_hr", "screentime_wknd_t_min")] <- lapply(abcd_screen_time[c("screentime_wkdy_typical_hr", "screentime_wkdy_typical_min", "screentime_wknd_typical_hr", "screentime_wknd_t_min")], as.integer)
abcd_screen_time$daily_screen_time <- apply(abcd_screen_time[c("screentime_wkdy_typical_hr", "screentime_wkdy_typical_min", "screentime_wknd_typical_hr", "screentime_wknd_t_min")], 1, function(x) {
if(sum(is.na(x[c(1,3)])) > 0 ) {
output <- NA
} else {
if(is.na(x[2])) x[2] <- 0
if(is.na(x[4])) x[4] <- 0
weekday_screen_time <- as.integer(x[1]) + (as.integer(x[2]) / 60)
weekend_day_screen_time <- as.integer(x[3]) + (as.integer(x[4]) / 60)
daily_screen_time <- ((weekday_screen_time * 5) + (weekend_day_screen_time * 2)) / 7
}
})
d2 <- bind_cols(
d2,
abcd_screen_time[, which(str_detect(tolower(names(abcd_screen_time)), "screen"))]
)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
# Chunk 2
# R packages
package_names <- c("dplyr", "kableExtra", "readr", "stringr")
load_packages <- lapply(package_names, require, character.only = TRUE)
# Chunk 3
# Import data from ABCD data table
get_abcd_data_table <- function(x) {
d <- read.table(paste0("z:\\", x), header = TRUE, fill = TRUE, sep = "\t", encoding = "UTF-8", stringsAsFactors = FALSE)
# Remove first row of metadata
d <- d[-1,]
}
# Get summed foreground minutes for a given app and participant id
get_minutes <- function(app, id) {
minutes <- d$foreground_minutes[d$participant_id == id & d$app_id == app]
if(length(minutes) == 0) 0 else sum(minutes)
}
# Convert 777 (refusal) and 999 (don't know) responses to NA
to_na <- function(x) {
x <- as.integer(x)
if(is.na(x)) output <- NA else if(x == 777 | x == 999) output <- NA else output <- x
output <- as.numeric(output)
}
# Chunk 4
# Michelle's app dictionary
dictionary <- read_csv("c:/users/joelb/onedrive/github/abcd/frequency_table_weighted_mg 02.20.2020.csv")
# Michelle's category lookup table
categories <- read_csv("c:/users/joelb/onedrive/github/abcd/categories.csv")
# Passive sensing raw data
d <- read_csv("c:/users/joelb/onedrive/github/abcd/passive_sensing_raw_data.csv")
# ABCD data tables
# Demographics (https://nda.nih.gov/data_structure.html?short_name=pdem02)
abcd_demographics <- get_abcd_data_table("pdem02.txt")
# Passive sensing data (https://nda.nih.gov/data_structure.html?short_name=abcd_earsraw01)
abcd_passive_sensing <- get_abcd_data_table("abcd_earsraw01.txt")
# Self-reported screen time questions (https://nda.nih.gov/data_structure.html?short_name=abcd_stq01)
abcd_screen_time <- get_abcd_data_table("abcd_stq01.txt") %>% filter(eventname == "2_year_follow_up_y_arm_1")
# Chunk 5
# Rename vectors
names(dictionary) <- unlist(dictionary[2,])
# Rename vectors with problematic names
names(dictionary)[c(5, 11, 12)] <- c("round_one", "abcd", "local_apk")
names(dictionary) <- gsub("|[[:digit:]]+", "", names(dictionary))
names(dictionary) <- gsub(" [(]| |-", "_", tolower(names(dictionary)))
names(dictionary) <- gsub("[)]", "", names(dictionary))
names(dictionary) <- gsub("\\_$", "", names(dictionary))
# Remove first two rows
dictionary <- dictionary[-c(1:2),]
# Remove select vectors
dictionary <- dictionary %>% select(-one_of(c("round_one", "local_apk")))
# Set elements 27, 39 and 45 to NA (duplicates) in the subgenre vector
dictionary$subgenre <- ifelse(dictionary$subgenre %in% c(27, 39, 45), NA, dictionary$subgenre)
# Coerce the minutes and props vectors to numeric objects
dictionary[c("minutes", "props")] <- lapply(dictionary[c("minutes", "props")], as.numeric)
# Coerce all but the first four vectors to integer objects
dictionary[-c(1:4)] <- lapply(dictionary[-c(1:4)], as.integer)
# Create a list of vectors that need to be created
vectors_to_create <- unname(unlist(sapply(names(dictionary)[-c(1:4)], function(x) {
names <- paste0(x, "_", sort(unique(eval(parse(text = paste0("dictionary$", x))))))
})))
# Chunk 6
# Rename vectors
names(categories) <- unlist(categories[1,])
# Rename vectors with problemtic names
names(categories)[4:6] <- c("games", "subgenre", "rating")
names(categories) <- gsub("|[[:digit:]]+", "", names(categories))
names(categories) <- gsub(" [(]| |-", "_", tolower(names(categories)))
names(categories) <- gsub("[)]", "", names(categories))
names(categories) <- gsub("\\_$", "", names(categories))
# Remove the first row and the third vector
categories <- categories[-c(1), -c(3)]
# Convert elements 27, 39 and 45 in the subgenre vector to NA (duplicates)
categories$subgenre[c(27, 39, 45)] <- NA
# Create a lookup table
variables <- labels <- c()
for(i in 1:ncol(categories)) {
for(j in 1:nrow(categories)) {
if(! is.na(categories[j, i])) {
explode <- strsplit(unlist(categories[j,i]), " - ")
variables <- append(variables, paste0(names(categories[i]), "_", explode[[1]][1]))
labels <- append(labels, gsub("[/]", "_", explode[[1]][2:(length(explode[[1]]))]))
}
}
}
lookup <- tibble(variables, labels)
# Coerce timestamp vectors to date objects
d[paste0("timestamp_", c("start", "stop"))] <- lapply(d[paste0("timestamp_", c("start", "stop"))], as.Date)
# Add vectors to the raw data corresponding to all of Michelle's categories
d[vectors_to_create] <- rep(NA, nrow(d))
# Iterate through these new vectors and update with foreground minutes if a code is found
for(i in 7:length(names(d))) {
explode <- strsplit(names(d)[i], "_")
variable <- paste0(explode[[1]][1:length(explode[[1]]) - 1], collapse = "_")
code <- as.integer(explode[[1]][length(explode[[1]])])
apps <- unname(unlist(dictionary %>% filter(!! sym(variable) == code) %>% select(app)))
eval(parse(text = paste0("d$", names(d)[i], " <- ifelse(d$app_id %in% apps, d$foreground_minutes, 0)")))
}
# Create aggregate raw data
d2 <-
bind_cols(
d %>%
group_by(participant_id) %>%
summarize(
start_date = min(timestamp_start),
stop_date = max(timestamp_stop),
days_of_data = length(unique(c(timestamp_start, timestamp_stop))),
daily_foreground_hours = round(sum(foreground_minutes) / 60 / days_of_data, 1),
),
d %>%
group_by(participant_id) %>%
summarise_at(vars(one_of(vectors_to_create)), sum) %>%
select(-participant_id)
)
# Create social media vectors
d2$Youtube <- sapply(unique(d$participant_id), function(x) get_minutes("com.google.android.youtube", x) )
d2$Tiktok <- sapply(unique(d$participant_id), function(x) get_minutes("com.zhiliaoapp.musically", x) )
d2$Snapchat <- sapply(unique(d$participant_id), function(x) get_minutes("com.snapchat.android", x) )
d2$Instagram <- sapply(unique(d$participant_id), function(x) get_minutes("com.instagram.android", x) )
d2$Netflix <- sapply(unique(d$participant_id), function(x) get_minutes("com.netflix.mediaclient", x) )
d2$Reddit <- sapply(unique(d$participant_id), function(x) get_minutes("com.reddit.frontpage", x) )
# Add ABCD demographic variables
abcd_demographics <- abcd_demographics[abcd_demographics$subjectkey %in% d2$participant_id,]
abcd_demographics <- abcd_demographics[match(d2$participant_id, abcd_demographics$subjectkey),]
abcd_demographics$demo_comb_income_v2 <- sapply(as.integer(as.character(abcd_demographics$demo_comb_income_v2)), function(x) to_na(x))
abcd_demographics$demo_prnt_ed_v2 <- sapply(as.integer(as.character(abcd_demographics$demo_prnt_ed_v2)), function(x) to_na(x))
abcd_demographics$demo_prtnr_ed_v2 <- sapply(as.integer(as.character(abcd_demographics$demo_prtnr_ed_v2)), function(x) to_na(x))
abcd_passive_sensing <- abcd_passive_sensing[abcd_passive_sensing$subjectkey %in% d2$participant_id,]
abcd_passive_sensing <- abcd_passive_sensing[match(d2$participant_id, abcd_passive_sensing$subjectkey),]
d2$Age <- as.integer(abcd_passive_sensing$interview_age) / 12
d2$Gender <- abcd_passive_sensing$sex
d2$`Family income` <- abcd_demographics$demo_comb_income_v2
d2$`Parent education` <- abcd_demographics$demo_prnt_ed_v2
d2$`Partner education` <- abcd_demographics$demo_prtnr_ed_v2
d2$`Max parental education` <- apply(
d2[c("Parent education", "Partner education")],
1,
function(x) {
if(sum(!is.na(x[1:2]), na.rm = TRUE) < 1 ) output <- NA else output <- max(x[1:2], na.rm = TRUE)
})
# Add ABCD self-reported screen time questions
abcd_screen_time <- abcd_screen_time[abcd_screen_time$subjectkey %in% d2$participant_id,]
abcd_screen_time <- abcd_screen_time[match(d2$participant_id, abcd_screen_time$subjectkey),]
abcd_screen_time[c("screentime_wkdy_typical_hr", "screentime_wkdy_typical_min", "screentime_wknd_typical_hr", "screentime_wknd_t_min")] <- lapply(abcd_screen_time[c("screentime_wkdy_typical_hr", "screentime_wkdy_typical_min", "screentime_wknd_typical_hr", "screentime_wknd_t_min")], as.integer)
abcd_screen_time$`Daily screen time` <- apply(abcd_screen_time[c("screentime_wkdy_typical_hr", "screentime_wkdy_typical_min", "screentime_wknd_typical_hr", "screentime_wknd_t_min")], 1, function(x) {
if(sum(is.na(x[c(1,3)])) > 0 ) {
output <- NA
} else {
if(is.na(x[2])) x[2] <- 0
if(is.na(x[4])) x[4] <- 0
weekday_screen_time <- as.integer(x[1]) + (as.integer(x[2]) / 60)
weekend_day_screen_time <- as.integer(x[3]) + (as.integer(x[4]) / 60)
daily_screen_time <- ((weekday_screen_time * 5) + (weekend_day_screen_time * 2)) / 7
}
})
d2 <- bind_cols(
d2,
abcd_screen_time[, which(str_detect(tolower(names(abcd_screen_time)), "screen"))]
)
# Convert select vectors from minutes to mean daily hours
d2[c(vectors_to_create, c("Youtube", "Tiktok", "Snapchat", "Instagram", "Netflix", "Reddit"))] <- round(d2[c(vectors_to_create, c("Youtube", "Tiktok", "Snapchat", "Instagram", "Netflix", "Reddit"))] / 60 / d2$days_of_data, 1)
# Rename the first five columns
names(d2)[1:5] <- c("Participant ID", "Start date", "Stop date", "Days of data", "Foreground time")
# Rename column names that appear in the lookup tibble
names(d2) <- sapply(names(d2), function(x) {
search_results <- lookup$labels[lookup$variables == x]
if(length(search_results) == 1) search_results else x
})
View(d2)
write.csv(d2, "c:/users/joelb/onedrive/github/abcd/passive-sensing-aggregate-data.csv", row.names = FALSE, na = "")
# Create table 1
tab1 <- d2[c(
"Age",
"Days of data",
"Daily screen time",
"Foreground time",
"Application",
"Games",
"Social media",
"Youtube",
"Tiktok",
"Snapchat",
"Instagram",
"Netflix",
"Reddit"
)]
# Compute summary statistics
tab1 <- bind_rows(
apply(tab1, 2, mean),
apply(tab1, 2, min),
apply(tab1, 2, max),
tab1
)
# Round numbers to two decimals
tab1 <- as_tibble(unlist(apply(tab1, 1:2, function(x) {
format(round(x, 2), nsmall = 2)
})))
# Add vectors
tab1 <- bind_cols(
tab1,
"Participant ID" = c("Mean", "Min", "Max", d2$`Participant ID`),
Gender = c(rep("", each = 3), d2$Gender)
)
# Reorder vectors
tab1 <- tab1[c("Participant ID", "Age", "Gender", "Days of data", names(tab1)[which(! names(tab1) %in% c("Participant ID", "Age", "Gender", "Days of data", "Start date", "Stop date"))])]
# Render table 1 in HTML
kable(tab1, align = c("l", rep("c", ncol(tab1) - 1))) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
row_spec(1:3, bold = T, color = "white", background = "#00AFBB")
citation("dplyr")
library(capl)
citation("capl")
library(lme4)
install.packages("lme4")
library(lme4)
citation("lme4")
install.packages("roxygen2")
library(roxygen2)
library(usethis)
install.packages("NCMisc")
install.packages("NCmisc")
# Load packages
package_names <- c("devtools", "NCmisc", "roxygen2")
load_packages <- lapply(package_names, require, character.only = TRUE)
# Create package
create(path = "c:/users/joelb/onedrive/github/capl")
citation("lme4")
detach("package:capl", unload = TRUE)
remove.packages("capl", lib="~/R/win-library/4.0")
detach("packages:capl")
rm(list = ls())
library(devtools)
library(stringr)
str_sort(c("a1", "a2", "a11"))
?str_sort
str_sort(c("a1", "a2", "a11"), numeric = TRUE)
as.Date("2020-01-25") + 359
a = 1:6
a[4:6] <- 0
a
library(capl)
data("capl_demo_data")
names(capl_demo_data)
sort(names(capl_demo_data))
mod <- lm(pacer_laps ~ age + sex, data = capl_demo_data)
mod <- lm(pacer_laps ~ age + gender, data = capl_demo_data)
mod
df <- tibble(
age = 13,
gender = "Boy"
)
df <- data.frame(
age = 13,
gender = "Boy"
)
?predict
predict(model, newdata = df)
predict(mod, newdata = df)
df <- data.frame(
age = 19,
gender = "Boy"
)
predict(mod, newdata = df)
df <- data.frame(
age = 19,
gender = "F"
)
predict(mod, newdata = df)
df <- data.frame(
age = 19,
gender = "F"
)
predict(mod, newdata = df)
df <- data.frame(
age = 19,
gender = "Female"
)
predict(mod, newdata = df)
df <- data.frame(
age = 19,
gender = "Male"
)
predict(mod, newdata = df)
df <- data.frame(
age = 12,
gender = "Male"
)
predict(mod, newdata = df)
mod <- lm(pacer_laps ~ age, data = capl_demo_data)
df <- data.frame(
age = 12,
gender = "Male"
)
predict(mod, newdata = df)
df <- data.frame(
age = 20,
gender = "Male"
)
predict(mod, newdata = df)
mod
df <- data.frame(
age = 50,
gender = "Male"
)
predict(mod, newdata = df)
mod
99 + (0.2 * 50)
library(stringr)
?str_sort
library(dplyr)
?is.tbl
a = tibble(x = 1:10)
is.tbl(a)
# Load dependencies
library(cansim); library(dplyr); library(plyr); library(stringr); library(readr); library(tidyr)
install.packages("cansim")
# Load dependencies
library(cansim); library(dplyr); library(plyr); library(stringr); library(readr); library(tidyr)
# Load dependencies
library(cansim); library(plyr); library(dplyr); library(stringr); library(readr); library(tidyr)
# Helper functions
# Wrangle the raw data
wrangle_data <- function(d) {
d_wide <- d %>% filter(`Episode week` != 99)
# Add leading zeros to case identifier number
d_wide$`Case identifier number` <- str_pad(d_wide$`Case identifier number`, width = nchar(max(as.numeric(d$`Case identifier number`))), pad = "0")
# Identify select vectors
vectors_to_factor <- c("Age group", "Gender", "Region", "Occupation", "Asymptomatic", "Transmission", "Hospital status", "Recovered", "Death")
# Restructure as factors
d_wide[vectors_to_factor] <- lapply(d_wide[vectors_to_factor], factor)
# Add semantic labels
d_wide$`Age group` <- revalue(d_wide$`Age group`, c("1" = "0-19", "2" = "20-29", "3" = "30-39", "4" = "40-49", "5" = "50-59", "6" = "60-69", "7" = "70-79", "8" = "80+", "99" = "Not stated"), warn_missing = FALSE)
d_wide$Gender <- revalue(d_wide$Gender, c("1" = "Male", "2" = "Female", "3" = "Non-binary", "7" = "Non-binary", "9" = "Not stated"), warn_missing = FALSE)
d_wide$Region <- revalue(d_wide$Region, c("1" = "Atlantic", "2" = "Quebec", "3" = "Ontario and Nunavut", "4" = "Prairies and the Northwest Territories", "5" = "British Columbia and Yukon"), warn_missing = FALSE)
d_wide$Occupation <- revalue(d_wide$Occupation, c("1" = "Health care worker", "2" = "School or daycare worker/attendee", "3" = "Long term care resident", "4" = "Other", "9" = "Not stated"), warn_missing = FALSE)
d_wide$Asymptomatic <- revalue(d_wide$Asymptomatic, c("1" = "Yes", "2" = "No", "9" = "Not stated"), warn_missing = FALSE)
d_wide$Transmission <- revalue(d_wide$Transmission, c("1" = "Domestic acquisition", "2" = "International travel", "9" = "Not stated"), warn_missing = FALSE)
d_wide$`Hospital status` <- revalue(d_wide$`Hospital status`, c("1" = "Hospitalized and in intensive care unit", "2" = "Hospitalized, but not in intensive care unit", "3" = "Not hospitalized", "9" = "Not stated/unknown"), warn_missing = FALSE)
d_wide$Recovered <- revalue(d_wide$Recovered, c("1" = "Yes", "2" = "No", "9" = "Not stated"), warn_missing = FALSE)
d_wide$Death <- revalue(d_wide$Death, c("1" = "Yes", "2" = "No", "9" = "Not stated"), warn_missing = FALSE)
# Add day (select first day of the week since not given), month and reference year vectors together and structure as a date object
d_wide$`Episode date` <- as.Date(paste(d_wide$`Episode year`, str_pad(d_wide$`Episode week`, width = 2, pad = 0), 1, sep = "-"), "%Y-%U-%u")
# Change format to %d-%b-%y
d_wide$`Episode date` <- strftime(d_wide$`Episode date`, format = "%d-%b-%y")
# Remove unwanted vectors from data
d_wide <- d_wide %>% select("Case identifier number", "Episode date", Gender, "Age group", "Region", "Occupation", Asymptomatic, Transmission, "Hospital status", Recovered, Death)
# Order data by case ids in ascending order
d_wide <- d_wide %>% arrange(`Case identifier number`)
return(d_wide)
}
# Set working directory
setwd("c:/users/joelb/onedrive/github/covid19")
paste0(getwd(), "/data/raw-data/", sort(list.files(paste0(getwd(), "/data/raw-data")), decreasing = TRUE)[1])
# Set working directory
setwd("c:/users/joelb/onedrive/github/covid19")
# Import data
d <- read_csv(paste0(getwd(), "/data/raw-data/", sort(list.files(paste0(getwd(), "/data/raw-data")), decreasing = TRUE)[1]))
# Wrangle data
new_snapshot <- wrangle_data(d)
# Convert `Episode Date` to date object
new_snapshot$`Episode date` <- as.Date(new_snapshot$`Episode date`, "%d-%b-%y")
# Create aggregate data
aggregate_data <- aggregate(
new_snapshot$`Case identifier number`,
list(
new_snapshot$`Episode date`,
new_snapshot$`Age group`,
new_snapshot$Gender,
new_snapshot$Region,
new_snapshot$Occupation,
new_snapshot$`Hospital status`,
new_snapshot$Death,
new_snapshot$Transmission
),
length
)
# Update names in aggregate_data
names(aggregate_data) <- c(
"Episode date",
"Age group",
"Gender",
"Region",
"Occupation",
"Hospital status",
"Death",
"Transmission",
"Counts"
)
# Convert factors to characters
aggregate_data[-c(1, ncol(aggregate_data))] <- lapply(aggregate_data[-c(1, ncol(aggregate_data))], as.character)
# Export data
saveRDS(aggregate_data, paste0("c:/users/joelb/onedrive/github/covid19/data/aggregate-data-", Sys.Date() ,".Rdata"), compress = "xz")
shiny::runApp('C:/Users/joelb/OneDrive/GitHub/covid19')
runApp('C:/Users/joelb/OneDrive/GitHub/covid19')
